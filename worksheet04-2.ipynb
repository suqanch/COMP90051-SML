{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 3\n",
    "## Regularization\n",
    "\n",
    "***\n",
    "\n",
    "In this worksheet, we'll explore regularization in the context of linear regression. Our key objectives are:\n",
    "\n",
    "* to implement basis expansion and extend the linear regression model to include polynomial basis functions\n",
    "* to understand the bias-variance trade-off and the role of regularization in controlling overfitting\n",
    "* to implement ridge regression and LASSO regression\n",
    "\n",
    "We'll reuse the Boston housing dataset from Workshop 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Boston Housing dataset\n",
    "\n",
    "Throughout this worksheet, we'll use the _Boston Housing dataset_ as an example. \n",
    "It contains data about towns in the Boston area, which can be used to predict median house values. \n",
    "There are 506 observations in total, each of which is described by 13 features, such as _per capita crime rate_, _percentage of population with a lower socio-economic status_, etc. \n",
    "You can read more about the features [here](http://lib.stat.cmu.edu/datasets/boston).\n",
    "\n",
    "Let's begin by loading the data (from scikit-learn) and converting to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "ds = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target, name='MEDV')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we'll work with a single feature called `LSTAT` for now. \n",
    "It corresponds to the percentage of the population in the town classified as 'lower status' by the US Census service in 1978. \n",
    "Note that the response variable (the median house value in the town) is denoted `MEDV`.\n",
    "Plotting the  `MEDV` vs. `LSTAT` we see that a linear model appears plausible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LSTAT']\n",
    "\n",
    "#ds['LSTAT'] = ds['LSTAT'].apply(lambda x: x/100.)\n",
    "for f in features:\n",
    "    plt.figure()\n",
    "    plt.scatter(ds[f], y, marker='.')\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(r'Median House Value ($\\times 10^3$ USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(ds, y, test_size=0.2, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "# select subset of the features\n",
    "X_train_s = X_train[features].values\n",
    "X_test_s = X_test[features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recap: Linear regression\n",
    "\n",
    "Recall that the linear regression model can be expressed as $\\hat{y} = \\mathbf{X}\\mathbf{w}$, where $\\mathbf{X}$ is the design matrix and $\\mathbf{w}$ is the weight vector.\n",
    "The optimal weights $\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend a column of 1's to the design matrices (since we absorbed the bias term in the weights vector)\n",
    "X_train_b = np.column_stack((np.ones_like(X_train_s), X_train_s))\n",
    "X_test_b = np.column_stack((np.ones_like(X_test_s), X_test_s))\n",
    "print('Design matrix shape:', X_train_s.shape)\n",
    "\n",
    "w = np.linalg.solve((X_train_b.T @ X_train_b), (X_train_b.T @ y_train)) # fill in matmul @ np.dot\n",
    "print('Weights:', w)\n",
    "\n",
    "# @ matmul np.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our implementation by plotting the predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"Return the predicted response for a given design matrix and weights vector\n",
    "    \"\"\"\n",
    "    return np.dot(X, w)\n",
    "\n",
    "X_grid = np.linspace(X_train_s.min(), X_train_s.max(), num=1001)\n",
    "x = np.column_stack((np.ones_like(X_grid), X_grid))\n",
    "y = predict(x, w)\n",
    "plt.plot(X_grid, y, 'k-', label='Prediction')\n",
    "plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "# plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel(\"$y$ (Median House Value)\")\n",
    "plt.xlabel(\"$x$ (LSTAT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute the mean error term over the training and test sets to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true)**2) \n",
    "\n",
    "y_pred_train = predict(X_train_b, w)\n",
    "y_pred_test = predict(X_test_b, w)\n",
    "print('Train MSE:', mean_squared_error(y_pred_train, y_train))\n",
    "print('Test MSE:', mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use the `sklearn` library to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train_s, y_train)\n",
    "y_pred_train = lr.predict(X_train_s)\n",
    "y_pred_test = lr.predict(X_test_s)\n",
    "print('Train MSE:', mean_squared_error(y_pred_train, y_train))\n",
    "print('Test MSE:', mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is obvious that the linear model is not a good fit for the data since the relationship between `LSTAT` and `MEDV` is not linear. What if we don't want to change our model? What could we do to capture the non-linear relationship using a 'linear' model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basis expansion\n",
    "\n",
    "Linear regression is simple and easy to interpret, however it cannot capture nonlinear relationships between the response and features. \n",
    "To deal with this problem, we can extend linear regression by mapping the features into another space where the relationship is (ideally) linear. \n",
    "This is known as _basis expansion_.\n",
    "Specifically, we map the original feature vector $\\mathbf{x} \\in \\mathbb{R}^m$ to a new feature vector $\\varphi(\\mathbf{x}) \\in \\mathbb{R}^k$. \n",
    "We then perform linear regression as before, replacing the original feature vectors with the new feature vectors: $y = w_0 + \\sum_{i = 1}^{m} w_i \\cdot \\varphi_i(\\mathbf{x})$. \n",
    "Note that this function is nonlinear in $\\mathbf{x}$, but linear in $\\mathbf{w}$. \n",
    "\n",
    "All of the previous results for simple linear regression carry over as you would expect by making the replacement $\\mathbf{x} \\to \\varphi(\\mathbf{x})$. \n",
    "For instance, the normal equation becomes: \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\star = \\left[\\mathbf{\\Phi}^\\top \\mathbf{\\Phi}\\right]^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Phi} = \\begin{pmatrix} \\varphi(\\mathbf{x}_1)^\\top \\\\ \\vdots \\\\ \\varphi(\\mathbf{x}_n)^\\top \\end{pmatrix}$ denotes the transformed design matrix.\n",
    "\n",
    "\n",
    "There are many possible choices for the mapping $\\varphi(\\mathbf{x})$, but we'll focus on using polynomial basis functions in the single-feature case, e.g. $\\varphi(x) = [1, x, x^2, \\ldots, x^{k - 1}]^\\top$ (note the first element corresponds to a bias term). \n",
    "\n",
    "We can compute the transformed design matrix using a built-in class from scikit-learn called `PolynomialFeatures`.\n",
    "We'll start by considering polynomial features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "Phi_train = poly.fit_transform(X_train_s)\n",
    "Phi_test = poly.fit_transform(X_test_s)\n",
    "print(\"Original design matrix (first 5 rows):\\n\", X_train_s[0:5], \"\\n\")\n",
    "print(\"Transformed design matrix (first 5 rows):\\n\", Phi_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform linear regression on the transformed training data and plot the resulting model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly = LinearRegression(fit_intercept=False).fit(Phi_train, y_train)\n",
    "\n",
    "X_grid = np.linspace(X_train_s.min(), X_train_s.max(), num=1001)\n",
    "Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "y = lr_poly.predict(Phi_grid)\n",
    "plt.plot(X_grid, y, 'k-', label='Prediction')\n",
    "plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel(\"$y$ (Median House Value)\")\n",
    "plt.xlabel(\"$x$ (LSTAT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a better fit than the linear model! Let's take a look at the error terms on the train/test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_poly = lr_poly.predict(Phi_train)\n",
    "y_pred_test_poly = lr_poly.predict(Phi_test)\n",
    "print('Train MSE for polynomial features of degree {}: {:.3f}'.format(degree, mean_squared_error(y_pred_train_poly, y_train)))\n",
    "print('Test MSE for polynomial features of degree {}: {:.3f}'.format(degree, mean_squared_error(y_pred_test_poly, y_test)))\n",
    "\n",
    "print('Train MSE using linear features only: {:.3f}'.format(mean_squared_error(lr.predict(X_train_s), y_train)))\n",
    "print('Test MSE using linear features only: {:.3f}'.format(mean_squared_error(lr.predict(X_test_s), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange, a large reduction on the train MSE but not so much on the test MSE. \n",
    "Lets scan across a range of powers. \n",
    "What do you expect to happen as we increase the maximum polynomial order on the training set? \n",
    "Take a minute to discuss with your fellow students before executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(12))\n",
    "models = list()\n",
    "train_mses = list()\n",
    "test_mses = list()\n",
    "\n",
    "X_grid = np.linspace(min(X_train_s.min(), X_test_s.min()), \n",
    "                     max(X_train_s.max(), X_test_s.max()), num=1001)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(len(degrees)//2, 2, i+1) \n",
    "    \n",
    "    # Transform features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    Phi_train, Phi_test = poly.fit_transform(X_train_s), poly.fit_transform(X_test_s)\n",
    "    Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "    \n",
    "    # Fit model\n",
    "    lr_poly = LinearRegression().fit(Phi_train, y_train)\n",
    "    models.append(lr_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = mean_squared_error(lr_poly.predict(Phi_train), y_train)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mse = mean_squared_error(lr_poly.predict(Phi_test), y_test)\n",
    "    test_mses.append(test_mse)\n",
    "    \n",
    "    plt.plot(X_grid, lr_poly.predict(Phi_grid), 'k', label='Prediction')\n",
    "    plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "    plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "    plt.title('Degree {} | Train MSE {:.3f}, Test MSE {:.3f}'.format(degree, train_mse, test_mse))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.suptitle('Polynomial regression for different polynomial degrees', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot mean-squared error vs. polynomial degree for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_mses, color='b', label='Train')\n",
    "plt.plot(degrees, test_mses, color='r', label='Test')\n",
    "plt.title('MSE vs. polynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question**: ðŸ¤¨ What's going on here? Does this match your earlier findings, or your intuition about which model was most appropriate? Why isn't test error behaving the same as training error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Ridge regression\n",
    "\n",
    "One solution for managing the bias-variance trade-off is regularisation. \n",
    "In the context of regression, one can simply add a penalty term to the least-squares cost function in order to encourage weight vectors that are sparse and/or small in magnitude.\n",
    "In this section, we'll experiment with ridge regression, where a $L_2$ (Tikhonov) penalty term is added to the cost function as follows:\n",
    "\n",
    "$$\n",
    "C(\\mathbf{w}) = \\| \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\|_2^2 + \\alpha \\| \\mathbf{w} \\|_2^2\n",
    "$$\n",
    "\n",
    "***\n",
    "**Exercise:** In lectures, we derived an analytic expression for the optimal weights $\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I} \\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$. Attempt the derivation yourself using the following matrix calculus identities: \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{w}^\\top \\mathbf{x} = \\nabla_{\\mathbf{w}} \\mathbf{x}^\\top \\mathbf{w} = \\mathbf{x}^\\top \\\\\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{A}\\mathbf{w} = \\mathbf{A} \\\\\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{w}^\\top \\mathbf{A}\\mathbf{w} = \\mathbf{w}^\\top \\left(\\mathbf{A}^\\top + \\mathbf{A}\\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "where vector $\\mathbf{x}$ and matrix $\\mathbf{A}$ are constants (independent of $\\mathbf{w}$).\n",
    "\n",
    "hint: $\\mathbf{w}^\\top \\mathbf{w} = \\mathbf{w}^\\top \\mathbf{I}\\mathbf{w}$\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining `alpha=0.002` and rescaling `LSTAT`. You'll need to rescale the `LSTAT` feature (e.g. divide by 100) in order to avoid numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = X_train_s / 100.0\n",
    "X_test_s = X_test_s / 100.0\n",
    "\n",
    "X_train_b = np.column_stack((np.ones_like(X_train_s), X_train_s))\n",
    "X_test_b = np.column_stack((np.ones_like(X_test_s), X_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise**: Implement ridge regression using the analytic expression for the optimal weights in the cell below.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.identity(X_train_s.shape[1])\n",
    "w = np.linalg.solve(np.dot(X_train_b.T, X_train_b + 0.002 * A), np.dot(X_train_b.T, y_train))\n",
    "print('Weights:', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could use the `Ridge` class from scikit-learn to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.002).fit(X_train_s, y_train)\n",
    "print('Weights:', ridge.coef_)\n",
    "print('Intercept:', ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then copy code from the previous section, making the replacement `LinearRegression()` -> `Ridge(alpha = 0.002)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(12))\n",
    "models = list()\n",
    "train_mses = list()\n",
    "test_mses = list()\n",
    "\n",
    "X_grid = np.linspace(min(X_train_s.min(), X_test_s.min()), \n",
    "                     max(X_train_s.max(), X_test_s.max()), num=1001)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(len(degrees)//2, 2, i+1) \n",
    "    \n",
    "    # Transform features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    Phi_train, Phi_test = poly.fit_transform(X_train_s), poly.fit_transform(X_test_s)\n",
    "    Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "    \n",
    "    # Fit model\n",
    "    lr_poly = Ridge(alpha = 0.002).fit(Phi_train, y_train)\n",
    "    models.append(lr_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = mean_squared_error(lr_poly.predict(Phi_train), y_train)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mse = mean_squared_error(lr_poly.predict(Phi_test), y_test)\n",
    "    test_mses.append(test_mse)\n",
    "    \n",
    "    plt.plot(X_grid, lr_poly.predict(Phi_grid), 'k', label='Prediction')\n",
    "    plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "    #plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "    plt.title('Degree {} | Train MSE {:.3f}'.format(degree, train_mse))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.suptitle('Polynomial ridge regression for different polynomial degrees', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model is no longer overfitting for larger polynomial degrees.\n",
    "We can confirm this by plotting the mean-squared error vs. the degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_mses, color='b', label='Train')\n",
    "plt.plot(degrees, test_mses, color='r', label='Test')\n",
    "plt.title('MSE vs. polynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot the L2 norm of the weights versus the polynomial degree. You should compare this with the non-regularized values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_L2 = [np.sum(m.coef_**2) for m in models]\n",
    "plt.plot(degrees, w_L2)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel(r'$\\| \\mathbf{w} \\|_2^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LASSO regression\n",
    "\n",
    "Similar to ridge regression, LASSO regression adds a penalty term to the least-squares cost function. However, LASSO uses a $L_1$ penalty term instead of a $L_2$ penalty term:\n",
    "\n",
    "$$\n",
    "C(\\mathbf{w}) = \\| \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\|_2^2 + \\alpha \\| \\mathbf{w} \\|_1\n",
    "$$\n",
    "\n",
    "The LASSO penalty term encourages sparsity in the weight vector, which can be useful for feature selection. Lasso encourages solutions to sit on the axes, therefore some of the weights are set to zero.\n",
    "\n",
    "However, unfortunately, there is no closed-form solution for the optimal weights in the LASSO case. We have to use numerical optimisation methods to solve the problem but solutions are sparse and can be found efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.002).fit(X_train_s, y_train)\n",
    "print('Weights:', lasso.coef_)\n",
    "print('Intercept:', lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then copy code from the previous section, making the replacement `Ridge(alpha = 0.002)` -> `Lasso(alpha = 0.002)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(12))\n",
    "models = list()\n",
    "train_mses = list()\n",
    "test_mses = list()\n",
    "\n",
    "X_grid = np.linspace(min(X_train_s.min(), X_test_s.min()), \n",
    "                     max(X_train_s.max(), X_test_s.max()), num=1001)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(len(degrees)//2, 2, i+1) \n",
    "    \n",
    "    # Transform features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    Phi_train, Phi_test = poly.fit_transform(X_train_s), poly.fit_transform(X_test_s)\n",
    "    Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "    \n",
    "    # Fit model\n",
    "    lr_poly = Lasso(alpha = 0.002).fit(Phi_train, y_train)\n",
    "    models.append(lr_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = mean_squared_error(lr_poly.predict(Phi_train), y_train)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mse = mean_squared_error(lr_poly.predict(Phi_test), y_test)\n",
    "    test_mses.append(test_mse)\n",
    "    \n",
    "    plt.plot(X_grid, lr_poly.predict(Phi_grid), 'k', label='Prediction')\n",
    "    plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "    #plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "    plt.title('Degree {} | Train MSE {:.3f}'.format(degree, train_mse))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.suptitle('Polynomial lasso regression for different polynomial degrees', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_mses, color='b', label='Train')\n",
    "plt.plot(degrees, test_mses, color='r', label='Test')\n",
    "plt.title('MSE vs. polynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot the L1 norm of the weights versus the polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_L2 = [np.sum(abs(m.coef_)) for m in models]\n",
    "plt.plot(degrees, w_L2)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel(r'$\\| \\mathbf{w} \\|_2^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Hyperparameter-tuning in regressions (this section is optional)\n",
    "\n",
    "You may want to experiment with different settings for the regularisation parameter $\\alpha$. \n",
    "How could you select the 'best' value of $\\alpha$ to achieve a good balance between training error and generalisation error?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
